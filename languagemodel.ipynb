{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1310a16f-0e66-4856-9cdc-e985baeef59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki.train.raw', 'r', encoding='utf-8') as file1:\n",
    "    corpus = file1.read()\n",
    "\n",
    "# Preprocess the text to remove punctuation\n",
    "corp1 = \"\"\n",
    "for char in corpus:\n",
    "    if char not in string.punctuation:\n",
    "        corp1 += char\n",
    "\n",
    "# Tokenizing\n",
    "words = corp1.lower().split()\n",
    "\n",
    "# Bigram\n",
    "def generate_bigrams(words):\n",
    "    bg_pairs = dict()\n",
    "    for i in range(len(words) - 1):\n",
    "        bg_pair = (words[i], words[i + 1])\n",
    "        bg_pairs[bg_pair] = bg_pairs.get(bg_pair, 0) + 1\n",
    "    return bg_pairs\n",
    "\n",
    "sentence_bigrams = generate_bigrams(words)\n",
    "\n",
    "for bigram, count in sentence_bigrams.items():\n",
    "    print(bigram, \":\", count)\n",
    "\n",
    "\n",
    "#Counts the no. of times a word repeats\n",
    "def freq_of_unique_words(words):\n",
    "    count = {}\n",
    "    corpus_word_count = 0\n",
    "    for word in words:\n",
    "        if word in count:\n",
    "            count[word] += 1\n",
    "        else:\n",
    "            count[word] = 1\n",
    "        corpus_word_count += 1  \n",
    "    \n",
    "    unique_word_count = len(count)\n",
    "\n",
    "    print(\"No of unique words in corpus:\", unique_word_count)\n",
    "    print(\"No of words in corpus:\", corpus_word_count)\n",
    "    \n",
    "    return count\n",
    "word_freq = freq_of_unique_words(words)\n",
    "#print(word_freq)\n",
    "\n",
    "def count_bigram_frequencies(words):\n",
    "    bigram_frequencies = defaultdict(int)\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = (words[i], words[i + 1])\n",
    "        bigram_frequencies[bigram] += 1\n",
    "    return bigram_frequencies\n",
    "    \n",
    "bigram_freq = count_bigram_frequencies(words)\n",
    "print(\"Number of unique bigrams:\", len(bigram_freq)) \n",
    "\n",
    "# Calculating bigram probability with Add-one smoothing\n",
    "def compute_bigram_probabilities(sentence_bigrams, word_freq):\n",
    "    V = len(word_freq)  \n",
    "    bigram_probabilities = {}\n",
    "    for bigram, freq in sentence_bigrams.items():\n",
    "        word, next_word = bigram\n",
    "        numerator = freq+1 \n",
    "        denominator = word_freq.get(word, 0) + V  \n",
    "        probability = numerator / denominator\n",
    "        bigram_probabilities[bigram] = probability\n",
    "    return bigram_probabilities\n",
    "\n",
    "bigram_probabilities = compute_bigram_probabilities(sentence_bigrams, word_freq)\n",
    "bigram_probabilities\n",
    "\n",
    "\n",
    "def compute_unigram_probabilities(word_freq):\n",
    "    unigram_probabilities = {}\n",
    "    total_word_count = sum(word_freq.values())\n",
    "    for word, freq in word_freq.items():\n",
    "        probability = (freq + 1) / (total_word_count + len(word_freq))  \n",
    "        unigram_probabilities[word] = probability\n",
    "    return unigram_probabilities\n",
    "\n",
    "\n",
    "def probability_with_backoff(bigram, bigram_probabilities, unigram_probabilities, backoff_weight=0.5):\n",
    "    if isinstance(bigram_probabilities, defaultdict):\n",
    "        if bigram in bigram_probabilities:\n",
    "            return bigram_probabilities[bigram]\n",
    "        else:\n",
    "            word, next_word = bigram\n",
    "            unigram_probability = unigram_probabilities.get(next_word, 1 / len(unigram_probabilities))\n",
    "            return backoff_weight * unigram_probability\n",
    "    else:  # If bigram_probabilities is a regular dictionary\n",
    "        return bigram_probabilities.get(bigram, 0)\n",
    "with open('wiki.valid.raw', 'r', encoding='utf-8') as file2:\n",
    "    valid = file2.read()\n",
    "corp2 = \"\"\n",
    "for char in valid:\n",
    "    if char not in string.punctuation:\n",
    "        corp2 += char\n",
    "val_data= corp2.lower().split()\n",
    "\n",
    "word_freq = freq_of_unique_words(val_data)\n",
    "#print(word_freq)\n",
    "sentence_bigrams = generate_bigrams(val_data)\n",
    "#bigram_freq = count_bigram_frequencies(train_data)\n",
    "bigram_freq = count_bigram_frequencies(val_data)\n",
    "\n",
    "unigram_probabilities = compute_unigram_probabilities(word_freq)\n",
    "bigram_probabilities = compute_bigram_probabilities(bigram_freq, word_freq)\n",
    "model = {\n",
    "    'unigram_probabilities': unigram_probabilities,\n",
    "    'bigram_probabilities': bigram_probabilities,\n",
    "}\n",
    "\n",
    "\n",
    "with open('wiki.test.raw', 'r', encoding='utf-8') as file3:\n",
    "    test = file3.read()\n",
    "corp3 = \"\"\n",
    "for char in test:\n",
    "  if char not in string.punctuation:\n",
    "     corp3 += char\n",
    "test_data = corp3.lower().split()\n",
    "\n",
    "def perplexity_without_backoff(words, model):\n",
    "    total_log_probability = 0\n",
    "    num_words = 0\n",
    "    \n",
    "    unigram_probabilities = model['unigram_probabilities']\n",
    "    bigram_probabilities = model['bigram_probabilities']\n",
    "    V = len(unigram_probabilities)\n",
    "    for i in range(len(test_data) - 1):\n",
    "        word, next_word = test_data[i], test_data[i + 1]\n",
    "        bigram_probability = bigram_probabilities.get((word, next_word), 0)\n",
    "        if bigram_probability == 0:\n",
    "            unigram_probability = unigram_probabilities.get(next_word, 1) / V\n",
    "            total_log_probability += math.log(unigram_probability)\n",
    "        else:\n",
    "            total_log_probability += math.log(bigram_probability)\n",
    "\n",
    "            num_words += 1\n",
    "    perplexity = math.exp(-total_log_probability / num_words)\n",
    "    \n",
    "    return perplexity\n",
    "    \n",
    "perplexity = perplexity_without_backoff(test_data, model)\n",
    "print(\"Perplexity:\",perplexity)\n",
    "\n",
    "def compute_bigram_probabilities_with_backoff(bigram_freq, unigram_probabilities, backoff_weight):\n",
    "    bigram_probabilities = {}\n",
    "    V = len(unigram_probabilities)\n",
    "    for bigram, freq in bigram_freq.items():\n",
    "        word, next_word = bigram\n",
    "        denominator = unigram_probabilities[word] if word in unigram_probabilities else 0\n",
    "        bigram_probabilities[bigram] = (freq + backoff_weight) / (denominator + V * backoff_weight)\n",
    "    return bigram_probabilities\n",
    "\n",
    "# Function to calculate perplexity with Backoff\n",
    "def perplexity_with_backoff(test_data, model):\n",
    "    total_log_probability = 0\n",
    "    num_words = 0\n",
    "    \n",
    "    unigram_probabilities = model['unigram_probabilities']\n",
    "    bigram_probabilities = model['bigram_probabilities']\n",
    "    backoff_weight = model.get('backoff_weight', 0.5)  \n",
    "    \n",
    "    V = len(unigram_probabilities)\n",
    "    \n",
    "    for i in range(len(test_data) - 1):\n",
    "        word, next_word = test_data[i], test_data[i + 1]\n",
    "        bigram_probability = bigram_probabilities.get((word, next_word), 0)\n",
    "        if bigram_probability == 0:\n",
    "            unigram_probability = unigram_probabilities.get(next_word, 1) / V\n",
    "            interpolated_probability = backoff_weight * unigram_probability\n",
    "            total_log_probability += math.log(interpolated_probability)\n",
    "        else:\n",
    "            total_log_probability += math.log(bigram_probability)\n",
    "        num_words += 1\n",
    "    \n",
    "    perplexity = math.exp(-total_log_probability / num_words)\n",
    "    return perplexity\n",
    "\n",
    "# Create models with and without Backoff\n",
    "model_without_backoff = {\n",
    "    'unigram_probabilities': unigram_probabilities,\n",
    "    'bigram_probabilities': bigram_probabilities,\n",
    "}\n",
    "\n",
    "model_with_backoff = {\n",
    "    'unigram_probabilities': unigram_probabilities,\n",
    "    'bigram_probabilities': compute_bigram_probabilities_with_backoff(bigram_freq, unigram_probabilities, 0.1),  # Using Backoff with a weight of 0.1\n",
    "}\n",
    "\n",
    "# Evaluate both models using the testing dataset\n",
    "perplexity_without_backoff_value = perplexity_without_backoff(test_data, model_without_backoff)\n",
    "perplexity_with_backoff_value = perplexity_with_backoff(test_data, model_with_backoff)\n",
    "\n",
    "# Compare perplexity scores\n",
    "print(\"Perplexity without Backoff:\", perplexity_without_backoff_value)\n",
    "print(\"Perplexity with Backoff:\", perplexity_with_backoff_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82107bb-37f0-4060-a339-4b8eaf29fc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
